{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/ccardenas93/lagtime.git","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:26:51.907594Z","iopub.execute_input":"2023-06-05T15:26:51.908354Z","iopub.status.idle":"2023-06-05T15:26:56.230382Z","shell.execute_reply.started":"2023-06-05T15:26:51.908306Z","shell.execute_reply":"2023-06-05T15:26:56.228666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import shutil\n#shutil.rmtree(\"/kaggle/working/lagtime\")","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:37:28.849680Z","iopub.execute_input":"2023-06-05T14:37:28.851418Z","iopub.status.idle":"2023-06-05T14:37:28.895577Z","shell.execute_reply.started":"2023-06-05T14:37:28.851365Z","shell.execute_reply":"2023-06-05T14:37:28.893835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.insert(0, \"/kaggle/working/lagtime/FCS\")\n\nimport fcs_functions as fcs\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:30:08.929037Z","iopub.execute_input":"2023-06-05T15:30:08.930241Z","iopub.status.idle":"2023-06-05T15:30:09.810173Z","shell.execute_reply.started":"2023-06-05T15:30:08.930193Z","shell.execute_reply":"2023-06-05T15:30:09.807301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install python-docx","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:30:12.793601Z","iopub.execute_input":"2023-06-05T15:30:12.794054Z","iopub.status.idle":"2023-06-05T15:30:31.677440Z","shell.execute_reply.started":"2023-06-05T15:30:12.794016Z","shell.execute_reply":"2023-06-05T15:30:31.675726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sys\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import TimeSeriesSplit\nimport os\n\n## font and color for ploting\nfont = {'family': 'serif', 'color':  'darkred', 'weight': 'normal', 'size': 14}\ncolor = [(174, 199, 232), (214, 39, 40)]  \n\nfor i in range(len(color)):    \n    r, g, b = color[i]    \n    color[i] = (r / 255., g / 255., b / 255.)  ","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:34:27.844726Z","iopub.execute_input":"2023-06-05T15:34:27.845263Z","iopub.status.idle":"2023-06-05T15:34:28.054318Z","shell.execute_reply.started":"2023-06-05T15:34:27.845219Z","shell.execute_reply":"2023-06-05T15:34:28.052965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Path of the gauges data (raingauges, surface and wastewater flow guages)\n##path to the csv data file\ninputfile1 = r'/kaggle/working/lagtime/FCS_input_files/9modeledStns.csv'\n##path to the csv file with station information\npath_sw = r'/kaggle/working/lagtime/FCS_input_files/wastewatercollectorslocation_new.csv'\n\n## dataframe from the CVS data file\ndf2 = pd.read_csv(inputfile1, sep=',', decimal='.', index_col=[0], parse_dates=True)\ndf2 = df2.sort_index(axis=1)\n\n\n# Read data about the guaging stations (ID, name, and x, y coordinates)\npath_rain = r'/kaggle/working/lagtime/FCS_input_files/Rainfall_guageing_stations.csv'\nRainguages = fcs.station_data(path_rain)\nSWguages = fcs.station_data(path_sw)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:39:24.403194Z","iopub.execute_input":"2023-06-05T15:39:24.403641Z","iopub.status.idle":"2023-06-05T15:40:02.496906Z","shell.execute_reply.started":"2023-06-05T15:39:24.403610Z","shell.execute_reply":"2023-06-05T15:40:02.495732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Create output folder\n\noutfolder=r'/kaggle/working/lagtime/Modelling_result'\nif not os.path.exists(outfolder):\n    os.makedirs(outfolder)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:41:35.454818Z","iopub.execute_input":"2023-06-05T15:41:35.455328Z","iopub.status.idle":"2023-06-05T15:41:35.461754Z","shell.execute_reply.started":"2023-06-05T15:41:35.455264Z","shell.execute_reply":"2023-06-05T15:41:35.460369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create list of Rainguages IDs\nrain = []\nfor rg in Rainguages:\n    rain.append(rg.id)\n    \n\n## Creat list of Surfacwwater flow uages IDs\nflow_st = []\nfor st in SWguages:\n    flow_st.append(st.id)\n\n\nAll_stn_results = pd.DataFrame()\n# Lis of stations to be modelled\nflow =  ['C01', 'C02', 'C11', 'U05', 'U06', 'U09', 'U11', 'U17', 'U19'] \n# flow =  ['C01' ]\n\n# lag_times = {'C01': 12,\n#              'C02': 15,\n#              'C11': 9,\n#              'U05': 15,\n#              'U06': 15,\n#              'U09': 15,\n#              'U11': 9,\n#              'U17': 18,\n#              'U19': 18}\n      \nnames = []\nrsquared = []\nWL = ['U06','C02','U09', 'C081','C082', 'C981','C982'] ## list of station of Water level\nn = 5 ## number of RF stations closest to the flow station considered\n","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:41:37.768031Z","iopub.execute_input":"2023-06-05T15:41:37.768485Z","iopub.status.idle":"2023-06-05T15:41:37.778081Z","shell.execute_reply.started":"2023-06-05T15:41:37.768452Z","shell.execute_reply":"2023-06-05T15:41:37.776612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create dataframe of the rainfall of the n number of closest stations\ndf = pd.DataFrame()\nfor f in flow:\n    print('station', f)\n    \n    if (f == st.id for st in SWguages):\n        df_list = fcs.closest_stations(Rainguages, SWguages, f,n)\n        #print('the flow station and the 10 closest RF stations are ' , df_list)\n        df_list = [x for x in df_list if x  in df2.columns.values]   #df2 is the dataframe containing the data read from the CSV\n        df = df2[df_list].copy() # the final dataframe for the DDM\n        names.append(f)\n    else:\n          print('No {0} in flow stations list'.format(f))  \n    \n    # ## if there are missing values, fill them with the mean of the series\n    # for col in df.columns.values:\n    #     df[col].fillna(df[col].mean(), inplace=True)\n    #     #df[col][df[col]<0] = df[col].mean()\n        \n    # abstract the data of the required length (here cosider only from 2016 to 2018)\n    df = df.loc['2016-01-01':'2018-12-31']\n   \n    \n #%%   \n    # get list of Rainfall stations associated with the flow station\n    rfst =  list(df.columns.values)\n    del rfst[0]  # the first in the list id the flow station\n    nshift = 24  ## the lag time in 5 minutes interval (2hr = 24, 1hr = 12) \n    rg_names = [] ## list of correlated rf gauging stations name\n    \n    corrST = []\n    maxcorr = []\n    indmaxcorr = [] ##index of the correlated RF station  \n    corrRG = [] \n    RF_back = 12    ## to increase the RF rolling sum duration\n    rolling_sum_window = nshift-RF_back\n    rolling_sum_windows =[3,6,9,12,15,18,21,24]\n    stn_result = pd.DataFrame()\n    for rolling_sum_window in rolling_sum_windows:\n        for rg in rfst:\n            #df[rg] = np.around(df[rg].rolling(nshift).apply(np.sum), decimals=2)\n            rg_lag = rg+'_rs'+str(rolling_sum_window)\n            # df[rg_lag] = df[rg].rolling(nshift).sum()\n            df[rg_lag] = np.around(df[rg].rolling(rolling_sum_window).apply(np.sum, raw=True), decimals=2)\n            corr = df[f].corr(df[rg_lag])\n            corrRG.append(corr)\n            rg_names.append(rg_lag)\n            sw2rg_corr = dict(zip(rg_names, corrRG))\n            \n        sorted_corr_list = sorted(sw2rg_corr, key=sw2rg_corr.get, reverse=True)\n    \n        # sorted_rg = [s.replace('_lag', '') for s in sorted_corr_list]\n        # sorted_rg=sorted_corr_list\n        nrfs2c = 5  ## number of RF stations to consider    \n        # df_rg = df[sorted_rg[:nrfs2c]]\n        df_rg = df[sorted_corr_list[:nrfs2c]]\n        if(df_rg.isnull().values.any()):\n            print('there is nan in the rainfall data')\n        \n    #%%\n         \n        # lag the flow series by nshift ( the duration of the forecast time)   \n        df_lag = pd.DataFrame()\n        for i in range(10):\n            #df[rg] = np.around(df[rg].rolling(nshift).apply(np.sum), decimals=2)\n            lag = 'lag_'+str(i)\n            name = f+'_lag_'+str(nshift+i)\n            df_lag[name] = df[f].shift(nshift+i)\n    \n        \n        df4 = pd.concat([df[f], df_lag], axis=1)\n        if(df4.isnull().values.any()):\n            print('there is nan in df[f]')\n    \n        if(not f in WL):\n            df4 = df4[df4>=0]\n    \n            \n        # add the 5 most correlated rainfall stations to the flow data frame\n        df4 = pd.concat([df4, df_rg], axis=1)\n            \n        if(df4.isnull().values.any()):\n            print('there is nan in df4')\n            \n        # drop the nans\n        df4 = df4.dropna()\n        \n        print('Information of the dataframe for the modelling:/n')\n        print(df4.info())\n        \n        # prepare the data for DDM\n        xcols =  list(df4.columns.values)\n        del xcols[0]   \n        indexData = df4.index.values\n        X = df4[xcols] #.values\n        y = df4[f] #.values\n        \n    \n        ## Training data size\n        splits = TimeSeriesSplit(4) ## 3/4 for training and 1/4 for testing\n        \n        for trainIdx, testIdx in splits.split(X):\n            trainIndex = trainIdx\n            testIndex = testIdx\n           \n    \n        X_train = X[:len(trainIndex)]\n        X_test = X[len(trainIndex): (len(trainIndex)+len(testIndex))]\n       \n        y_train = y[:len(trainIndex)]\n        y_test = y[len(trainIndex): (len(trainIndex)+len(testIndex))]\n       \n        print('Observations: %d' % (len(X_train)+len(X_test)))\n        print('Training Observations: %d' % len(X_train))\n        print('Testing Observations: %d' % len(X_test))\n    \n          ## normilize the input features      \n        minMaxScaler = MinMaxScaler(feature_range=(0, 1))\n        # X_train[X_train.columns] = minMaxScaler.fit_transform(X_train)\n        # X_test[X_test.columns] = minMaxScaler.transform(X_test)\n        X_train = minMaxScaler.fit_transform(X_train)\n        X_test = minMaxScaler.transform(X_test)\n        \n        \n        \n        # ## transforming the data to better represent peaks\n        # # standardizing\n        # X_train, mean, std =fcs.standardize_fit(X_train)\n        # X_test = fcs.standardize(X_test, mean, std)\n        \n        # # normalizing\n        # X_train, xmax, xmin =fcs.normalize_fit(X_train)\n        # X_test = fcs.normalize(X_test, xmax, xmin)\n        \n        \n        \n    #    #Wilson_Hilferty_transformation_fit\n    #    X_train, mean, std = Wilson_Hilferty_transformation_fit(X_train)\n    #    X_test = Wilson_Hilferty_transformation(X_test, mean, std)\n        \n        \n        \n        ## Lists to hold the evaluation criterial and model\n        results = []\n        models = []\n        \n        ## train the modl\n        predictions_train, predictions_test, RF \\\n            = fcs.fit_RF(X_train, X_test, y_train, y_test)\n            \n        # # save the trained model\n        # fcs.pickle_out(RF)\n        \n        ## create dataframes of training and testing resulsts for ploting\n        pred_train = pd.Series(predictions_train, y_train.index)\n        df_train = pd.concat([y_train, pred_train], axis=1)\n        df_train.columns =['observed', 'estimated']\n        \n        pred_test = pd.Series(predictions_test, y_test.index)\n        df_test = pd.concat([y_test, pred_test], axis=1)\n        df_test.columns =['observed', 'estimated']\n        \n        ## evaluate the training and test of the model    \n        r2_train, rmse_train, nse_train = fcs.evaluation_stat(y_train,predictions_train)\n        r2_test, rmse_test, nse_test = fcs.evaluation_stat(y_test,predictions_test)\n          \n        ## Plot the resulsts\n        \n        \n        # df_train = df_train.loc['2016-01-01':'2016-01-02']\n        gaps = 1 ## to plt the data with gaps where there are NANs\n        fig3 = fcs.plot_regression_result('RandomForest_Training', f,df_train,r2_train,color,gaps)\n        fig4 = fcs.plot_regression_result('RandomForest_Test',f,df_test,r2_test,color,gaps)\n            \n        score = ['RF',r2_train, r2_test, rmse_train, rmse_test,nse_train, nse_test]\n        results.append(score)\n            \n        models.append(RF)\n        plots_list = [fig3, fig4]\n        \n        ##create resultas dataframe\n        columnNames = ['Method','Rsqured_train','Rsquared_test','rmse_train','rmse_test','NSE_train', 'NSE_test']\n        result = pd.DataFrame(results, columns=columnNames)\n        \n        ## save the results, figures and model in a word file\n        input_list = X.columns.values\n        input_train_len, input_features_len = X_train.shape\n        input_test_len,_ = X_test.shape\n        lag = nshift*5  ## lag time in minutes\n    \n        fcs.wirte_results_summary_to_word2(f,lag,input_train_len, input_features_len,\n                                           input_list, input_test_len, \n                                           plots_list,result, models,rolling_sum_window,outfolder\n                                           )\n        \n        score_stn = [f+'_'+str(rolling_sum_window),r2_train, r2_test, rmse_train, rmse_test,nse_train, nse_test]\n        columnNames_stn = ['Station','Rsqured_train','Rsquared_test','rmse_train','rmse_test','NSE_train', 'NSE_test']\n        results_stn = []\n        results_stn.append(score_stn)\n        result_stn_df = pd.DataFrame(results_stn, columns=columnNames_stn)\n        stn_result = pd.concat([stn_result, result_stn_df],axis=0)\n        All_stn_results = pd.concat([All_stn_results, result_stn_df],axis=0)\n    fcs.wirte_all_stn_results(stn_result,outfolder,f)\n# All_stn_results.sort_values(\"Station\", inplace=True)\nfcs.wirte_all_stn_results(All_stn_results,outfolder,'All stations with different rolling sum window')\noutcsv = r'/kaggle/working/lagtime/Modelling_resultU05_different_Rollingsum sindwo.csv'\nwith open(outcsv,'w') as outfile:\n        All_stn_results.to_csv(outfile, float_format='%.3f', sep=';',line_terminator='/n')\n        \ndf_stn = {}      \nlag_times = {}\nfor f in flow: \n    dfname = f+'df'\n    df_stn[dfname] = All_stn_results[All_stn_results['Station'].str.contains(f)]\n    df_stn[dfname].sort_values(['Rsquared_test', 'Rsqured_train'], ascending=[False, False],inplace=True)\n    lag_times[f] = int(df_stn[dfname]['Station'].iloc[0].replace(f+'_', ''))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:41:40.364904Z","iopub.execute_input":"2023-06-05T15:41:40.365366Z","iopub.status.idle":"2023-06-05T15:47:32.773308Z","shell.execute_reply.started":"2023-06-05T15:41:40.365320Z","shell.execute_reply":"2023-06-05T15:47:32.771493Z"},"trusted":true},"execution_count":null,"outputs":[]}]}